name: CI Pipeline

on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  AWS_REGION: us-east-2
  AWS_ROLE_ARN: arn:aws:iam::143519759870:role/GitHubActionsMLOpsRole
  AWS_ACCOUNT_ID: "143519759870"

jobs:
  validate-and-build:
    runs-on: ubuntu-latest

    permissions:
      id-token: write # Required for OIDC authentication
      contents: read # Required to checkout code

    steps:
      # Step 1: Checkout repository
      - name: Checkout code
        uses: actions/checkout@v4

      # Step 2: Set up Python
      - name: Set up Python 3.11
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      # Step 3: Install Python dependencies for testing
      - name: Install test dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      # Step 4: Validate services configuration
      # This checks if all service directories exist and have required files
      - name: Validate project structure
        run: |
          echo "Validating microservices structure..."
          for service in inference-service feedback-service model-registry-service evaluation-service retraining-service notification-service model-init-service api-gateway-service; do
            if [ ! -d "services/$service" ]; then
              echo "ERROR: Missing service directory: $service"
              exit 1
            fi
            if [ ! -f "services/$service/Dockerfile" ]; then
              echo "ERROR: Missing Dockerfile for: $service"
              exit 1
            fi
            if [ ! -f "services/$service/requirements.txt" ]; then
              echo "ERROR: Missing requirements.txt for: $service"
              exit 1
            fi
            echo "‚úì $service validated"
          done
          echo "All services validated successfully!"

      # Step 5: Validate Docker Compose configuration
      - name: Validate docker-compose.yml
        run: |
          echo "Validating docker-compose configuration..."
          docker compose config > /dev/null
          echo "‚úì docker-compose.yml is valid"

      # Step 6: Build all Docker images
      # This ensures all services can be containerized successfully
      - name: Build Docker images
        run: |
          echo "Building all microservices..."
          docker compose build
          echo "‚úì All Docker images built successfully"

      # Step 7: List built images for verification
      - name: List Docker images
        run: |
          echo "Built images:"
          docker images | grep -E "(ml-sentiment|inference|feedback|registry|evaluation|retraining|notification|init|gateway)"

      # Step 8: Configure AWS credentials using OIDC
      # This uses GitHub's OIDC provider to assume an IAM role
      # No static credentials needed!
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-CI-${{ github.run_id }}

      # Step 9: Verify AWS authentication
      # Confirms that OIDC authentication was successful
      - name: Verify AWS identity
        run: |
          echo "Verifying AWS credentials..."
          aws sts get-caller-identity
          echo "‚úì AWS authentication successful"

      # Step 10: Run basic AWS connectivity tests
      # Ensures the assumed role has necessary permissions
      - name: Test AWS permissions
        run: |
          echo "Testing AWS S3 access..."
          aws s3 ls || echo "‚ö† S3 access not configured (expected for initial setup)"

          echo "Testing AWS SageMaker access..."
          aws sagemaker list-training-jobs --max-results 1 || echo "‚ö† SageMaker access not configured (expected for initial setup)"

          echo "Testing AWS ECR access..."
          aws ecr describe-repositories || echo "‚ö† ECR access not configured (expected for initial setup)"

      # Step 11: Clean up Docker resources
      - name: Clean up
        if: always()
        run: |
          echo "Cleaning up Docker resources..."
          docker compose down --volumes --remove-orphans || true
          docker system prune -f || true

  # CD Job: Push images to ECR (runs only on main branch push)
  push-to-ecr:
    runs-on: ubuntu-latest
    needs: validate-and-build
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-CD-${{ github.run_id }}

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repositories if not exist
        run: |
          for service in inference-service feedback-service model-registry-service evaluation-service retraining-service notification-service model-init-service api-gateway-service; do
            aws ecr describe-repositories --repository-names ml-sentiment-$service 2>/dev/null || \
            aws ecr create-repository --repository-name ml-sentiment-$service --image-scanning-configuration scanOnPush=true
            echo "‚úì Repository ml-sentiment-$service ready"
          done

      - name: Build and push Docker images to ECR
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "Building and pushing images with tag: $IMAGE_TAG"

          # Build and push each service
          for service in inference-service feedback-service model-registry-service evaluation-service retraining-service notification-service model-init-service api-gateway-service; do
            echo "Processing $service..."
            
            # Build image
            docker build -t ml-sentiment-$service:latest ./services/$service
            
            # Tag for ECR
            docker tag ml-sentiment-$service:latest $ECR_REGISTRY/ml-sentiment-$service:$IMAGE_TAG
            docker tag ml-sentiment-$service:latest $ECR_REGISTRY/ml-sentiment-$service:latest
            
            # Push to ECR
            docker push $ECR_REGISTRY/ml-sentiment-$service:$IMAGE_TAG
            docker push $ECR_REGISTRY/ml-sentiment-$service:latest
            
            echo "‚úì $service pushed to ECR"
          done

          echo "All images pushed successfully!"

      - name: Output image URIs
        env:
          ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
          IMAGE_TAG: ${{ github.sha }}
        run: |
          echo "Images pushed to ECR:"
          echo "----------------------------------------"
          for service in inference-service feedback-service model-registry-service evaluation-service retraining-service notification-service model-init-service api-gateway-service; do
            echo "$service: $ECR_REGISTRY/ml-sentiment-$service:$IMAGE_TAG"
          done

  # Infrastructure Deployment with Terraform
  deploy-infrastructure:
    runs-on: ubuntu-latest
    needs: push-to-ecr
    if: github.ref == 'refs/heads/main' && github.event_name == 'push'

    permissions:
      id-token: write
      contents: read

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v2
        with:
          terraform_version: 1.5.0

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ env.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
          role-session-name: GitHubActions-Terraform-${{ github.run_id }}

      - name: Upload training data to S3
        run: |
          echo "Uploading training data to S3..."
          aws s3 cp Dataset/train_data.csv s3://ml-sentiment-data-${{ env.AWS_ACCOUNT_ID }}/train_data.csv --region ${{ env.AWS_REGION }} || echo "Bucket may not exist yet, will be created by Terraform"

      - name: Upload SageMaker scripts to S3
        run: |
          echo "Uploading SageMaker training scripts to S3..."
          aws s3 sync sagemaker-scripts/ s3://ml-sentiment-models-${{ env.AWS_ACCOUNT_ID }}/sagemaker-scripts/ --region ${{ env.AWS_REGION }} || echo "Bucket may not exist yet, will be created by Terraform"

      - name: Terraform Init
        working-directory: ./infrastructure
        run: |
          echo "Initializing Terraform..."
          terraform init

      - name: Check for stale locks
        working-directory: ./infrastructure
        run: |
          echo "Checking for stale Terraform locks..."
          # Check if lock exists and is older than 1 hour
          LOCK_INFO=$(aws dynamodb scan \
            --table-name ml-sentiment-terraform-locks \
            --region us-east-2 \
            --query 'Items[*].[LockID.S,Info.S]' \
            --output json 2>/dev/null || echo "[]")

          if [ "$LOCK_INFO" != "[]" ] && [ -n "$LOCK_INFO" ]; then
            echo "‚ö†Ô∏è Found existing lock(s). Terraform will retry with backoff."
            echo "If this persists, manually unlock: terraform force-unlock <LOCK_ID>"
          else
            echo "‚úì No stale locks found"
          fi

      - name: Terraform Validate
        working-directory: ./infrastructure
        run: |
          echo "Validating Terraform configuration..."
          terraform validate

      - name: Terraform Plan
        working-directory: ./infrastructure
        env:
          TF_VAR_image_tag: ${{ github.sha }}
        run: |
          echo "Planning infrastructure changes..."
          # Retry with lock timeout handling
          max_attempts=3
          attempt=1
          while [ $attempt -le $max_attempts ]; do
            if terraform plan -out=tfplan; then
              echo "‚úì Terraform plan successful"
              break
            else
              if [ $attempt -lt $max_attempts ]; then
                echo "‚ö† Plan failed (attempt $attempt/$max_attempts), waiting 30s and retrying..."
                sleep 30
                attempt=$((attempt + 1))
              else
                echo "‚ùå Terraform plan failed after $max_attempts attempts"
                # Try to force unlock if locked
                terraform force-unlock -force $(terraform show -json 2>/dev/null | jq -r '.values.root_module.resources[] | select(.type == "aws_dynamodb_table") | .values.id' 2>/dev/null || echo "") 2>/dev/null || true
                exit 1
              fi
            fi
          done

      - name: Terraform Apply
        working-directory: ./infrastructure
        env:
          TF_VAR_image_tag: ${{ github.sha }}
        run: |
          echo "Applying infrastructure changes..."
          # Retry with exponential backoff for lock handling
          max_attempts=5
          attempt=1
          wait_time=10

          while [ $attempt -le $max_attempts ]; do
            if terraform apply -auto-approve tfplan -lock-timeout=5m; then
              echo "‚úì Terraform apply successful"
              break
            else
              EXIT_CODE=$?
              if [ $EXIT_CODE -eq 1 ] && [ $attempt -lt $max_attempts ]; then
                echo "‚ö† Apply failed (attempt $attempt/$max_attempts), waiting ${wait_time}s and retrying..."
                sleep $wait_time
                wait_time=$((wait_time * 2))  # Exponential backoff
                attempt=$((attempt + 1))
              else
                echo "‚ùå Terraform apply failed after $max_attempts attempts"
                exit $EXIT_CODE
              fi
            fi
          done

      - name: Upload training data (retry after S3 creation)
        run: |
          echo "Ensuring training data is uploaded..."
          aws s3 cp Dataset/train_data.csv s3://ml-sentiment-data-${{ env.AWS_ACCOUNT_ID }}/training/train_data.csv --region ${{ env.AWS_REGION }}
          echo "‚úì Training data uploaded"

      - name: Get deployment outputs
        working-directory: ./infrastructure
        run: |
          echo "==========================================="
          echo "üöÄ DEPLOYMENT COMPLETE!"
          echo "==========================================="
          echo ""
          echo "Load Balancer URL:"
          terraform output -raw alb_url
          echo ""
          echo ""
          echo "API Endpoints:"
          echo "  Health:   $(terraform output -raw alb_url)/health"
          echo "  Predict:  $(terraform output -raw alb_url)/predict"
          echo "  Feedback: $(terraform output -raw alb_url)/feedback"
          echo "  Models:   $(terraform output -raw alb_url)/models"
          echo ""
          echo "S3 Buckets:"
          echo "  Models: $(terraform output -raw s3_models_bucket)"
          echo "  Data:   $(terraform output -raw s3_data_bucket)"
          echo ""
          echo "SageMaker Endpoint:"
          terraform output -raw sagemaker_endpoint_name
          echo ""
          echo "==========================================="

      - name: Test health endpoints
        run: |
          echo "Waiting 60 seconds for services to start..."
          sleep 60

          echo "Testing ALB health endpoint..."
          ALB_URL=$(cd infrastructure && terraform output -raw alb_url)

          for i in {1..10}; do
            if curl -f "$ALB_URL/health" 2>/dev/null; then
              echo "‚úì Health check passed!"
              exit 0
            fi
            echo "Attempt $i failed, retrying in 30 seconds..."
            sleep 30
          done

          echo "‚ö† Services are starting (this may take a few minutes)"
          echo "‚ö† Check ECS console for service status"
